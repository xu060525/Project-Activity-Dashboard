import altair as alt
import streamlit as st
import pandas as pd
import traceback
from datetime import datetime

from github_loader import GitHubLoader
from db_manager import DBManager
from score_calculator import calculate_health_score
from classifier import CommitClassifier
from ai_analyst import AIAnalyst

# === åˆå§‹åŒ– ===
st.set_page_config(page_title="Project Activity Dashboard", layout="wide") # ç¨å¾®ä¼˜åŒ–ä¸€ä¸‹å¸ƒå±€å®½å±æ¨¡å¼
st.title("Project Activity Dashboard")

# 1. åˆå§‹åŒ– Session State (è¿™æ˜¯è§£å†³ä½ é—®é¢˜çš„æ ¸å¿ƒï¼)
# æˆ‘ä»¬ç”¨è¿™ä¸¤ä¸ªå˜é‡æ¥â€œè®°ä½â€æ•°æ®ï¼Œä¸ç®¡ä½ ç‚¹ä»€ä¹ˆæŒ‰é’®ï¼Œæ•°æ®éƒ½åœ¨ã€‚
if 'data' not in st.session_state:
    st.session_state['data'] = None
if 'current_repo' not in st.session_state:
    st.session_state['current_repo'] = ""

# å®‰å…¨æ£€æŸ¥
st.subheader("Security Check")
try:
    token = st.secrets["GITHUB_TOKEN"]
except:
    st.error("Please config your Token in .streamlit/secrets.toml !")
    st.stop()

# åˆå§‹åŒ–åŠ è½½å™¨
loader = GitHubLoader(token)
db = DBManager()
classifier = CommitClassifier()

# === ç”¨æˆ·è¾“å…¥ ===
# è¿™é‡Œæˆ‘ä»¬åªæ˜¯èŽ·å–è¾“å…¥ï¼Œä¸ç«‹å³æ‰§è¡Œ
repo_input = st.text_input("Enter Repository Name", "pandas-dev/pandas")

# === é€»è¾‘å— Aï¼šèŽ·å–æ•°æ® (Data Fetching) ===
# åªæœ‰ç‚¹å‡»è¿™ä¸ªæŒ‰é’®æ—¶ï¼Œæ‰åŽ»ç½‘ç»œè¯·æ±‚å’Œè¯»æ•°æ®åº“
if st.button("Analyze Project"):
    with st.spinner(f"Fetching data from {repo_input} ..."):
        try:
            # æ›´æ–°å½“å‰åˆ†æžçš„ä»“åº“å
            st.session_state['current_repo'] = repo_input
            
            # --- æ ¸å¿ƒæ•°æ®èŽ·å–é€»è¾‘ (å’Œä½ åŽŸæ¥çš„ä¸€æ ·) ---
            last_date = db.get_latest_commit_date(repo_input)

            if last_date:
                st.info(f"Local data found. Last update: {last_date}. Fetching new commits...")
                new_commits_raw = loader.fetch_commits(repo_input, limit=500, since_date=last_date)
            else:
                st.info("First time analysis. Fetching history...")
                new_commits_raw = loader.fetch_commits(repo_input, limit=500)

            # æ•°æ®æ¸…æ´— & åˆ†ç±»
            cleaned_data = []
            for c in new_commits_raw:
                message = c['commit']['message']
                category = classifier.classify(message)
                cleaned_data.append({
                    "sha": c['sha'],
                    "author": c['commit']['author']['name'],
                    "date": c['commit']['author']['date'],
                    "message": c['commit']['message'],
                    "additions": 0,
                    "deletions": 0,
                    "category": category
                })

            if cleaned_data:
                db.save_commits(repo_input, cleaned_data)

            # è¯»å–å…¨é‡æ•°æ®
            all_commits = db.get_all_commits(repo_input)
            df = pd.DataFrame(all_commits)
            
            # --- ã€å…³é”®ä¿®æ”¹ã€‘ ---
            # ä¸è¦åœ¨è¿™é‡Œç›´æŽ¥ç”»å›¾ï¼
            # è€Œæ˜¯æŠŠå¤„ç†å¥½çš„ df å­˜è¿› Session State é‡Œ
            st.session_state['data'] = df
            
            # æˆåŠŸåŽï¼Œå¼ºåˆ¶åˆ·æ–°ä¸€ä¸‹é¡µé¢ï¼Œè®©ä¸‹é¢çš„ "é€»è¾‘å— B" ç«‹å³è¿è¡Œ
            st.rerun()

        except Exception as e:
            st.error(f"Error during fetch: {e}")
            traceback.print_exc()


# === é€»è¾‘å— Bï¼šå±•ç¤ºæ•°æ® (Data Visualization) ===
# åªè¦ Session State é‡Œæœ‰æ•°æ®ï¼Œè¿™éƒ¨åˆ†ä»£ç å°±ä¼šè¿è¡Œã€‚
# æ— è®ºä½ æ˜¯åˆšç‚¹å®Œ "Analyze"ï¼Œè¿˜æ˜¯ç‚¹äº† "AI Report"ï¼Œè¿™é‡Œéƒ½ä¼šæ‰§è¡Œï¼
# === é€»è¾‘å— Bï¼šå±•ç¤ºæ•°æ® (Data Visualization) ===
if st.session_state['data'] is not None:
    
    # ä½¿ç”¨å‰¯æœ¬ï¼Œé˜²æ­¢ä¿®æ”¹åŽŸå§‹æ•°æ®
    df = st.session_state['data'].copy()
    repo_name = st.session_state['current_repo']

    try:
        # === æ•°æ®é¢„å¤„ç† ===
        if 'date' in df.columns:
            df['date'] = pd.to_datetime(df['date'], utc=True)
            df.set_index('date', inplace=True)
            df.sort_index(inplace=True)

        st.success(f"Analysis Ready for {repo_name}! Total Commits: {len(df)}")
        
        # === Day 3 æ–°å¢ž: ä½¿ç”¨ Tabs ç»„ç»‡å¸ƒå±€ ===
        tab_overview, tab_deep_dive, tab_intent = st.tabs(["ðŸš€ Overview", "ðŸ“ˆ Deep Dive", "ðŸ§  Intent Analysis"])
        
        # è®¡ç®—ä¸€äº›é€šç”¨æŒ‡æ ‡ (å¤ç”¨)
        total_commits = len(df)
        contributors = df['author'].nunique()
        duration = (df.index.max() - df.index.min()).days if not df.empty else 0
        score, reasons = calculate_health_score(df)
        
        # --- Tab 1: Overview (æ€»è§ˆ) ---
        with tab_overview:
            # 1. æ ¸å¿ƒæŒ‡æ ‡å¡ç‰‡
            col1, col2, col3 = st.columns(3)
            col1.metric("Total Commits", total_commits)
            col2.metric("Contributors", contributors)
            col3.metric("Active Days", f"{duration} days")
            
            st.divider()
            
            # 2. å¥åº·åº¦è¯„åˆ† (å·¦) + AI æŠ¥å‘Š (å³)
            col_score, col_ai = st.columns([1, 2])
            
            with col_score:
                st.subheader("Health Score")
                color = "green" if score >= 80 else "orange" if score >= 50 else "red"
                st.markdown(f"""
                    <div style="text-align: center; padding: 20px; border: 1px solid #ddd; border-radius: 10px;">
                        <h1 style="color: {color}; font-size: 80px; margin:0;">{score}</h1>
                        <p>Out of 100</p>
                    </div>
                """, unsafe_allow_html=True)
                
                # æ˜¾ç¤ºç®€å•çš„æ‰£åˆ†åŽŸå› 
                with st.expander("View Score Details"):
                    for reason in reasons:
                        st.write(reason)

            with col_ai:
                st.subheader("AI CTO Diagnosis")
                
                # å‡†å¤‡ AI æ•°æ®
                type_counts = df['category'].value_counts()
                intent_dist = type_counts.to_dict()
                is_risky = (len(reasons) > 0 and "Risk" in reasons[-1])
                
                # è®¡ç®—è¶‹åŠ¿
                weekly_commits = df.resample('W')['sha'].count()
                if len(weekly_commits) >= 4:
                    recent_avg = weekly_commits.tail(4).mean()
                    total_avg = weekly_commits.mean()
                    trend = "Rising" if recent_avg > total_avg * 1.2 else "Falling" if recent_avg < total_avg * 0.5 else "Stable"
                else:
                    trend = "Unknown"

                # ç¼“å­˜é€»è¾‘ (Day 2 ä½œä¸šæˆæžœ)
                if 'ai_reports' not in st.session_state:
                    st.session_state['ai_reports'] = {}
                current_report = st.session_state['ai_reports'].get(repo_name, "")
                
                if current_report:
                    st.info(current_report)
                    if st.button("Regenerate Diagnosis", key="regen_btn"):
                        st.session_state['ai_reports'][repo_name] = ""
                        st.rerun()
                else:
                    if st.button("Generate AI Report", key="gen_btn"):
                        try:
                            api_key = st.secrets["DEEPSEEK_API_KEY"]
                            analyst = AIAnalyst(api_key)
                            with st.spinner("AI is analyzing..."):
                                stream = analyst.generate_assessment(repo_name, score, is_risky, intent_dist, trend)
                                
                                # éžæµå¼å¤„ç† (å¦‚æžœ ai_analyst æ”¹äº†çš„è¯) æˆ– æµå¼å¤„ç†
                                # è¿™é‡Œå‡è®¾ä½ ç”¨çš„æ˜¯ Day 2 çš„æµå¼ä»£ç 
                                report_box = st.empty()
                                full_resp = ""
                                if isinstance(stream, str):
                                    st.error(stream)
                                else:
                                    for chunk in stream:
                                        if chunk.choices and chunk.choices[0].delta.content:
                                            full_resp += chunk.choices[0].delta.content
                                            report_box.markdown(full_resp + "â–Œ")
                                    
                                    report_box.markdown(full_resp)
                                    st.session_state['ai_reports'][repo_name] = full_resp
                        except Exception as e:
                            st.error(f"AI Error: {e}")

        # --- Tab 2: Deep Dive (æ·±åº¦åˆ†æž) ---
        with tab_deep_dive:
            st.subheader("Commit Activity")
            st.line_chart(weekly_commits)
            
            col_d1, col_d2 = st.columns(2)
            
            with col_d1:
                st.subheader("Work Rhythm")
                day_counts = df.index.day_name().value_counts()
                days_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']
                day_counts = day_counts.reindex(days_order, fill_value=0).reset_index()
                day_counts.columns = ['Day', 'Commits']
                
                c = alt.Chart(day_counts).mark_bar().encode(
                    x=alt.X('Day', sort=days_order), y='Commits', tooltip=['Day', 'Commits']
                )
                st.altair_chart(c, use_container_width=True)
                
            with col_d2:
                st.subheader("Top Contributors")
                author_counts = df['author'].value_counts().head(10).reset_index()
                author_counts.columns = ['Author', 'Commits']
                
                c2 = alt.Chart(author_counts).mark_bar().encode(
                    x=alt.X('Author', sort='-y'), y='Commits', color=alt.Color('Commits', legend=None)
                )
                st.altair_chart(c2, use_container_width=True)

        # --- Tab 3: Intent Analysis (æ„å›¾åˆ†æž) ---
        with tab_intent:
            col_i1, col_i2 = st.columns([2, 1])
            
            with col_i1:
                st.subheader("Category Distribution")
                type_df = type_counts.reset_index()
                type_df.columns = ['Category', 'Count']
                
                base = alt.Chart(type_df).encode(theta=alt.Theta("Count", stack=True))
                pie = base.mark_arc(outerRadius=120).encode(
                    color=alt.Color("Category"),
                    order=alt.Order("Count", sort="descending"),
                    tooltip=["Category", "Count"]
                )
                st.altair_chart(pie, use_container_width=True)
                
            with col_i2:
                st.subheader("Data Quality")
                other_count = type_df[type_df['Category'] == 'Other']['Count'].sum()
                ratio = other_count / total_commits if total_commits > 0 else 0
                st.metric("Unclassified Ratio", f"{ratio:.1%}")
                
                if ratio > 0.3:
                    st.warning("High unclassified ratio. Consider updating classification rules.")
                else:
                    st.success("Good data quality.")

    except Exception as e:
        st.error(f"Visualization Error: {e}")
        traceback.print_exc()